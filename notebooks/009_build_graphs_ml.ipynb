{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Building graphs with Metric Learning (and an introduction to the new pytorch lightning-based framework)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "\n",
    "from gnn_tracking.training.ml import MLModule\n",
    "from gnn_tracking.models.graph_construction import GraphConstructionFCNN\n",
    "from gnn_tracking.metrics.losses import GraphConstructionHingeEmbeddingLoss\n",
    "from pytorch_lightning import Trainer\n",
    "from gnn_tracking.utils.loading import TrackingDataModule\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch import nn\n",
    "from pytorch_lightning.core.mixins import HyperparametersMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Configuring the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The configuration for train/val/test data and its dataloader is held in the `TrackingDataModule` (subclass of `LightningDataModule`)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "dm = TrackingDataModule(\n",
    "    train=dict(\n",
    "        dirs=[\"/Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/\"],\n",
    "        stop=5,\n",
    "    ),\n",
    "    val=dict(\n",
    "        dirs=[\"/Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/\"],\n",
    "        start=5,\n",
    "        stop=10,\n",
    "    ),\n",
    "    # could also configure a 'test' set here\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Other keys allow to configure the loaders (batch size, number of workers, etc.). See the docstring of `TrackingDataModule` for details."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Details (for understanding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that all of the following will be done implicitly by the `Trainer` and you won't have to worry about it. But if you want to inspect the data, you can do so.\n",
    "\n",
    "When calling the `setup` method, the `LightningDataModule` initializes instances of `TrackingDataset` (`torch_geometric.Dataset`) for each of these. We can get the corresponding dataloaders by calling `dm.train_dataloader()` and analog for validation and test.\n",
    "\n",
    "Example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[13:00:44] INFO: DataLoader will load 5 graphs (out of 90 available).\u001B[0m\n",
      "\u001B[36m[13:00:44] DEBUG: First graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21025_s0.pt, last graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21053_s0.pt\u001B[0m\n",
      "\u001B[32m[13:00:44] INFO: DataLoader will load 5 graphs (out of 90 available).\u001B[0m\n",
      "\u001B[36m[13:00:44] DEBUG: First graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21058_s0.pt, last graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21094_s0.pt\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'train': TrackingDataset(5), 'val': TrackingDataset(5)}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is called by the Trainer automatically and sets up the datasets\n",
    "dm.setup(stage=\"fit\")  # 'fit' combines 'train' and 'val'\n",
    "# Now the datasets are available:\n",
    "dm.datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For example, we can inspect the first element of the training dataset:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "data = dm.datasets[\"train\"][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get the corresponding dataloaders, use one of the methods (but again, you probalby won't need to):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(<torch_geometric.loader.dataloader.DataLoader at 0x147d2f090>,\n <torch_geometric.loader.dataloader.DataLoader at 0x15815d950>)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_dataloader(), dm.val_dataloader()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Configuring a model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We write a normal `torch.nn.Module`. The easiest way is to import one of the modules that we have already written in the `gnn_tracking` librar."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "model = GraphConstructionFCNN(in_dim=14, out_dim=8, depth=5, hidden_dim=64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, you can also write your own. Here is a very simple one:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class DemoGraphConstructionModel(nn.Module, HyperparametersMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden_dim: int,\n",
    "        out_dim: int,\n",
    "        depth: int = 5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # This is made available by the HyperparametersMixin\n",
    "        # all of our hyperparameters from the __init__ arguments\n",
    "        # are saved to self.hparams (but we don't need this in this\n",
    "        # example)\n",
    "        self.save_hyperparameters()\n",
    "        assert depth > 2\n",
    "        _layers = [\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "        for _ in range(depth - 2):\n",
    "            _layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            _layers.append(nn.ReLU())\n",
    "        _layers.append(nn.Linear(hidden_dim, out_dim))\n",
    "        self._model = nn.Sequential(*_layers)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        # Our trainer class will expect us to return a dictionary, where\n",
    "        # the key H has the transformed latent space.\n",
    "        return {\"H\": self._model(data.x)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# model = DemoGraphConstructionModel(in_dim=14, out_dim=8, hidden_dim=64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you are familiar with normal pytorch, there was only few differences:\n",
    "\n",
    "1. We inherit from `HyperparamsMixin`\n",
    "2. We call `self.save_hyperparameters()`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Details (for understanding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We saved all hyperparameters:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "\"beta\":       0.4\n\"depth\":      5\n\"hidden_dim\": 64\n\"in_dim\":     14\n\"out_dim\":    8"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note how `depth=5` was saved despite not being specified explicitly (it was recognized as a default parameter)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As always, you can simply evaluate the `model` on a piece of data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "out = model(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Configuring loss functions, metrics and the lightning module"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The pytorch model is bundled together with a set of loss functions (just one here), that we backpropagate from in the training step, and a set of metrics. Together, these components make up the `LightningModule` that we pass to the pytorch lightning `Trainer` for training.\n",
    "\n",
    "If you were familiar with our previous `TCNTrainer` training class, this `MLModule` now fulfills (almost) the exact same role."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m[13:03:20] DEBUG: Got obj of type <class '__main__.DemoGraphConstructionModel'>, assuming I have to save hyperparameters\u001B[0m\n",
      "\u001B[36m[13:03:20] DEBUG: Saving hyperperameters {'class_path': '__main__.DemoGraphConstructionModel', 'init_args': {'in_dim': 14, 'hidden_dim': 64, 'out_dim': 8, 'depth': 5}}\u001B[0m\n",
      "\u001B[36m[13:03:20] DEBUG: Got obj of type <class 'gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss'>, assuming I have to save hyperparameters\u001B[0m\n",
      "\u001B[36m[13:03:20] DEBUG: Saving hyperperameters {'class_path': 'gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss', 'init_args': {'r_emb': 0.002, 'max_num_neighbors': 10, 'attr_pt_thld': 0.9, 'p_attr': 1, 'p_rep': 1}}\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "lmodel = MLModule(\n",
    "    model=model,\n",
    "    loss_fct=GraphConstructionHingeEmbeddingLoss(max_num_neighbors=10),\n",
    "    lw_repulsive=0.5,  # loss weight, see below\n",
    "    optimizer=partial(torch.optim.Adam, lr=1e-4),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Details (for understanding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, all hyperparameters are accessible (even the ones that weren't explicitly specified but only set by default):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "\"loss_fct\":     {'class_path': 'gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss', 'init_args': {'r_emb': 0.002, 'max_num_neighbors': 10, 'attr_pt_thld': 0.9, 'p_attr': 1, 'p_rep': 1}}\n\"lw_repulsive\": 0.5\n\"model\":        {'class_path': '__main__.DemoGraphConstructionModel', 'init_args': {'in_dim': 14, 'hidden_dim': 64, 'out_dim': 8, 'depth': 5}}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmodel.hparams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, any _objects_ that were passed to the model are also saved to the hyperparameters in a way that we can bring them back."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The loss function takes output from the model and the data and returns two separate losses:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "{'attractive': tensor(2.0991, grad_fn=<DivBackward0>),\n 'repulsive': tensor(2.5671e-06, grad_fn=<SumBackward0>)}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fct = GraphConstructionHingeEmbeddingLoss()\n",
    "loss_fct(\n",
    "    x=out[\"H\"],\n",
    "    particle_id=data.particle_id,\n",
    "    batch=data.batch,\n",
    "    edge_index=data.edge_index,\n",
    "    pt=data.pt,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Both parts of the loss functions are combined with the loss weight we have configured above (weight of 1 for attractive, weight of 0.5 for repulsive). All of this is done in `MLModule.get_losses` (returning the total loss and a dictionary of the individual losses):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(2.0991, grad_fn=<AddBackward0>),\n {'attractive': 2.09908127784729,\n  'repulsive': 2.5670772174635204e-06,\n  'attractive_weighted': 2.09908127784729,\n  'repulsive_weighted': 1.2835386087317602e-06})"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmodel.get_losses(out, data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\u001B[32m[13:08:53] INFO: DataLoader will load 5 graphs (out of 90 available).\u001B[0m\n",
      "\u001B[36m[13:08:53] DEBUG: First graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21025_s0.pt, last graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21053_s0.pt\u001B[0m\n",
      "\u001B[32m[13:08:53] INFO: DataLoader will load 5 graphs (out of 90 available).\u001B[0m\n",
      "\u001B[36m[13:08:53] DEBUG: First graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21058_s0.pt, last graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21094_s0.pt\u001B[0m\n",
      "\n",
      "  | Name     | Type                                | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | model    | DemoGraphConstructionModel          | 14.0 K\n",
      "1 | loss_fct | GraphConstructionHingeEmbeddingLoss | 0     \n",
      "-----------------------------------------------------------------\n",
      "14.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "14.0 K    Total params\n",
      "0.056     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1372c5f3b7c74f3e9299d43b1f8d7790"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 6789. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8459. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "\n\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[3m           Validation epoch=0            \u001B[0m\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mMetric             \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m  Value\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mError\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━┩\n│ attractive          │ 2.12333 │   nan │\n│ attractive_weighted │ 2.12333 │   nan │\n│ repulsive           │ 0.00000 │   nan │\n│ repulsive_weighted  │ 0.00000 │   nan │\n│ total               │ 2.12333 │   nan │\n└─────────────────────┴─────────┴───────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">           Validation epoch=0            </span>\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Metric              </span>┃<span style=\"font-weight: bold\">   Value </span>┃<span style=\"font-weight: bold\"> Error </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━┩\n│ attractive          │ 2.12333 │   nan │\n│ attractive_weighted │ 2.12333 │   nan │\n│ repulsive           │ 0.00000 │   nan │\n│ repulsive_weighted  │ 0.00000 │   nan │\n│ total               │ 2.12333 │   nan │\n└─────────────────────┴─────────┴───────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92b7ea946c59495e93b62520a347c1dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed743307af2640fd89368a352a0a69dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 7299. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 5946. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8207. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "\n\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[3m           Validation epoch=0            \u001B[0m\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mMetric             \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m  Value\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mError\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━┩\n│ attractive          │ 1.94851 │   nan │\n│ attractive_weighted │ 1.94851 │   nan │\n│ repulsive           │ 0.00000 │   nan │\n│ repulsive_weighted  │ 0.00000 │   nan │\n│ total               │ 1.94851 │   nan │\n└─────────────────────┴─────────┴───────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">           Validation epoch=0            </span>\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Metric              </span>┃<span style=\"font-weight: bold\">   Value </span>┃<span style=\"font-weight: bold\"> Error </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━┩\n│ attractive          │ 1.94851 │   nan │\n│ attractive_weighted │ 1.94851 │   nan │\n│ repulsive           │ 0.00000 │   nan │\n│ repulsive_weighted  │ 0.00000 │   nan │\n│ total               │ 1.94851 │   nan │\n└─────────────────────┴─────────┴───────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=1, accelerator=\"cpu\", log_every_n_steps=1)\n",
    "trainer.fit(model=lmodel, datamodule=dm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Restoring a pre-trained model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take a look at the `lightning_logs` directory:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m\u001B[36mversion_0\u001B[m\u001B[m   \u001B[1m\u001B[36mversion_118\u001B[m\u001B[m \u001B[1m\u001B[36mversion_22\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_42\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_62\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_82\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_1\u001B[m\u001B[m   \u001B[1m\u001B[36mversion_119\u001B[m\u001B[m \u001B[1m\u001B[36mversion_23\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_43\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_63\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_83\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_10\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_12\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_24\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_44\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_64\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_84\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_100\u001B[m\u001B[m \u001B[1m\u001B[36mversion_120\u001B[m\u001B[m \u001B[1m\u001B[36mversion_25\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_45\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_65\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_85\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_101\u001B[m\u001B[m \u001B[1m\u001B[36mversion_121\u001B[m\u001B[m \u001B[1m\u001B[36mversion_26\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_46\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_66\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_86\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_102\u001B[m\u001B[m \u001B[1m\u001B[36mversion_122\u001B[m\u001B[m \u001B[1m\u001B[36mversion_27\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_47\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_67\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_87\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_103\u001B[m\u001B[m \u001B[1m\u001B[36mversion_123\u001B[m\u001B[m \u001B[1m\u001B[36mversion_28\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_48\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_68\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_88\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_104\u001B[m\u001B[m \u001B[1m\u001B[36mversion_124\u001B[m\u001B[m \u001B[1m\u001B[36mversion_29\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_49\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_69\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_89\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_105\u001B[m\u001B[m \u001B[1m\u001B[36mversion_125\u001B[m\u001B[m \u001B[1m\u001B[36mversion_3\u001B[m\u001B[m   \u001B[1m\u001B[36mversion_5\u001B[m\u001B[m   \u001B[1m\u001B[36mversion_7\u001B[m\u001B[m   \u001B[1m\u001B[36mversion_9\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_106\u001B[m\u001B[m \u001B[1m\u001B[36mversion_126\u001B[m\u001B[m \u001B[1m\u001B[36mversion_30\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_50\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_70\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_90\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_107\u001B[m\u001B[m \u001B[1m\u001B[36mversion_127\u001B[m\u001B[m \u001B[1m\u001B[36mversion_31\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_51\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_71\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_91\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_108\u001B[m\u001B[m \u001B[1m\u001B[36mversion_128\u001B[m\u001B[m \u001B[1m\u001B[36mversion_32\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_52\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_72\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_92\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_109\u001B[m\u001B[m \u001B[1m\u001B[36mversion_13\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_33\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_53\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_73\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_93\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_11\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_14\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_34\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_54\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_74\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_94\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_110\u001B[m\u001B[m \u001B[1m\u001B[36mversion_15\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_35\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_55\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_75\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_95\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_111\u001B[m\u001B[m \u001B[1m\u001B[36mversion_16\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_36\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_56\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_76\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_96\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_112\u001B[m\u001B[m \u001B[1m\u001B[36mversion_17\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_37\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_57\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_77\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_97\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_113\u001B[m\u001B[m \u001B[1m\u001B[36mversion_18\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_38\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_58\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_78\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_98\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_114\u001B[m\u001B[m \u001B[1m\u001B[36mversion_19\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_39\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_59\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_79\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_99\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_115\u001B[m\u001B[m \u001B[1m\u001B[36mversion_2\u001B[m\u001B[m   \u001B[1m\u001B[36mversion_4\u001B[m\u001B[m   \u001B[1m\u001B[36mversion_6\u001B[m\u001B[m   \u001B[1m\u001B[36mversion_8\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_116\u001B[m\u001B[m \u001B[1m\u001B[36mversion_20\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_40\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_60\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_80\u001B[m\u001B[m\r\n",
      "\u001B[1m\u001B[36mversion_117\u001B[m\u001B[m \u001B[1m\u001B[36mversion_21\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_41\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_61\u001B[m\u001B[m  \u001B[1m\u001B[36mversion_81\u001B[m\u001B[m\r\n"
     ]
    }
   ],
   "source": [
    "! ls lightning_logs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0-step=5.ckpt\r\n"
     ]
    }
   ],
   "source": [
    "! ls lightning_logs/version_0/checkpoints"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Navigate to one of the versions and take a look at the `hparams.yaml` file. It should contain exactly the hyperparameters from the run.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\r\n",
      "  class_path: gnn_tracking.models.graph_construction.GraphConstructionFCNN\r\n",
      "  init_args:\r\n",
      "    in_dim: 14\r\n",
      "    hidden_dim: 64\r\n",
      "    out_dim: 8\r\n",
      "    depth: 5\r\n",
      "    beta: 0.4\r\n",
      "lw_repulsive: 0.5\r\n",
      "loss_fct:\r\n",
      "  class_path: gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss\r\n",
      "  init_args:\r\n",
      "    r_emb: 0.002\r\n",
      "    max_num_neighbors: 10\r\n",
      "    attr_pt_thld: 0.9\r\n",
      "    p_attr: 1\r\n",
      "    p_rep: 1\r\n"
     ]
    }
   ],
   "source": [
    "! cat lightning_logs/version_0/hparams.yaml"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can bring back the trained model by loading one of the checkpoints:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m[19:39:17] DEBUG: Got {'class_path': 'gnn_tracking.models.graph_construction.GraphConstructionFCNN', 'init_args': {'in_dim': 14, 'hidden_dim': 64, 'out_dim': 8, 'depth': 5, 'beta': 0.4}}, assuming I have to load\u001B[0m\n",
      "\u001B[36m[19:39:17] DEBUG: Getting class GraphConstructionFCNN from module gnn_tracking.models.graph_construction\u001B[0m\n",
      "\u001B[36m[19:39:17] DEBUG: Got {'class_path': 'gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss', 'init_args': {'r_emb': 0.002, 'max_num_neighbors': 10, 'attr_pt_thld': 0.9, 'p_attr': 1, 'p_rep': 1}}, assuming I have to load\u001B[0m\n",
      "\u001B[36m[19:39:17] DEBUG: Getting class GraphConstructionHingeEmbeddingLoss from module gnn_tracking.metrics.losses\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "restored_model = MLModule.load_from_checkpoint(\n",
    "    \"lightning_logs/version_0/checkpoints/epoch=0-step=5.ckpt\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note how we didn't have to specify any the hyperparameters again.\n",
    "\n",
    "However, we can easily change some of them by adding them as additional keyword arguments."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m[19:40:11] DEBUG: Got {'class_path': 'gnn_tracking.models.graph_construction.GraphConstructionFCNN', 'init_args': {'in_dim': 14, 'hidden_dim': 64, 'out_dim': 8, 'depth': 5, 'beta': 0.4}}, assuming I have to load\u001B[0m\n",
      "\u001B[36m[19:40:11] DEBUG: Getting class GraphConstructionFCNN from module gnn_tracking.models.graph_construction\u001B[0m\n",
      "\u001B[36m[19:40:11] DEBUG: Got obj of type <class 'gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss'>, assuming I have to save hyperparameters\u001B[0m\n",
      "\u001B[36m[19:40:11] DEBUG: Saving hyperperameters {'class_path': 'gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss', 'init_args': {'r_emb': 0.002, 'max_num_neighbors': 5, 'attr_pt_thld': 0.9, 'p_attr': 1, 'p_rep': 1}}\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "restored_model_modified = MLModule.load_from_checkpoint(\n",
    "    \"lightning_logs/version_0/checkpoints/epoch=0-step=5.ckpt\",\n",
    "    lw_repulsive=0.1,\n",
    "    loss_fct=GraphConstructionHingeEmbeddingLoss(max_num_neighbors=5),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that you cannot modify the model architecture however (but you could in principle change the `beta` parameter of the residual connections)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running all of this from the command line"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All of the following can be achieved by running the following command:\n",
    "\n",
    "```bash\n",
    "python3 gnn_tracking/trainers/run.py fit --model configs/model.yml --data configs/data.yml  --trainer.accelerator cpu --trainer.accelerator cpu\n",
    "```\n",
    "\n",
    "with the data config file\n",
    "\n",
    "```yaml\n",
    "train:\n",
    "  dirs:\n",
    "    - /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/\n",
    "  stop: 5\n",
    "test:\n",
    "  dirs:\n",
    "    - /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/\n",
    "  star: 10\n",
    "  stop: 15\n",
    "val:\n",
    "  dirs:\n",
    "    - /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/\n",
    "  start: 5\n",
    "  stop: 10\n",
    "```\n",
    "\n",
    "and model config file:\n",
    "\n",
    "```\n",
    "class_path: gnn_tracking.training.ml.MLModule\n",
    "init_args:\n",
    "  model:\n",
    "    class_path: gnn_tracking.models.graph_construction.GraphConstructionFCNN\n",
    "    init_args:\n",
    "      in_dim: 14\n",
    "      out_dim: 8\n",
    "      hidden_dim: 512\n",
    "      depth: 5\n",
    "  lw_repulsive: 0.5\n",
    "  loss_fct:\n",
    "    class_path: gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss\n",
    "    init_args: {}\n",
    "  optimizer:\n",
    "    class_path: torch.optim.Adam\n",
    "    init_args:\n",
    "      lr: 0.0001\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To quickly override one of the options, you can simply add them to the command line, e.g., `--model.init_args.lw_repulsive=0.1` or `--model.model.init_args.depth=6`."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
