{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Building graphs with Metric Learning\n",
    "\n",
    "This notebook shows how to build graphs using a metric learning strategy. For this, every hit is independently projected to a latent space using a fully connected neural network. The network is trained to put hits from the same particle close to each other and hits from different particles far from each other. An initial graph can then be constructed by connecting hits that are close in this space.\n",
    "This strategy has been adapted by ExaTrkx, see for example [section 5.2 here.](https://link.springer.com/10.1140/epjc/s10052-021-09675-8)\n",
    "\n",
    "This notebook also serves as an introduction to the new pytorch lightning-based framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "\n",
    "from gnn_tracking.training.ml import MLModule\n",
    "from gnn_tracking.models.graph_construction import GraphConstructionFCNN\n",
    "from gnn_tracking.metrics.losses import GraphConstructionHingeEmbeddingLoss\n",
    "from pytorch_lightning import Trainer\n",
    "from gnn_tracking.utils.loading import TrackingDataModule\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch import nn\n",
    "from pytorch_lightning.core.mixins import HyperparametersMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration for train/val/test data and its dataloader is held in the `TrackingDataModule` (subclass of `LightningDataModule`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = TrackingDataModule(\n",
    "    train=dict(\n",
    "        dirs=[\"/Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/\"],\n",
    "        stop=5,\n",
    "    ),\n",
    "    val=dict(\n",
    "        dirs=[\"/Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/\"],\n",
    "        start=5,\n",
    "        stop=10,\n",
    "    ),\n",
    "    # could also configure a 'test' set here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other keys allow to configure the loaders (batch size, number of workers, etc.). See the docstring of `TrackingDataModule` for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details (for understanding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all of the following will be done implicitly by the `Trainer` and you won't have to worry about it. But if you want to inspect the data, you can do so.\n",
    "\n",
    "When calling the `setup` method, the `LightningDataModule` initializes instances of `TrackingDataset` (`torch_geometric.Dataset`) for each of these. We can get the corresponding dataloaders by calling `dm.train_dataloader()` and analog for validation and test.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10:06:48] INFO: DataLoader will load 5 graphs (out of 90 available).\u001b[0m\n",
      "\u001b[36m[10:06:48] DEBUG: First graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21025_s0.pt, last graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21053_s0.pt\u001b[0m\n",
      "\u001b[32m[10:06:48] INFO: DataLoader will load 5 graphs (out of 90 available).\u001b[0m\n",
      "\u001b[36m[10:06:48] DEBUG: First graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21058_s0.pt, last graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21094_s0.pt\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': TrackingDataset(5), 'val': TrackingDataset(5)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is called by the Trainer automatically and sets up the datasets\n",
    "dm.setup(stage=\"fit\")  # 'fit' combines 'train' and 'val'\n",
    "# Now the datasets are available:\n",
    "dm.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can inspect the first element of the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dm.datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the corresponding dataloaders, use one of the methods (but again, you probalby won't need to):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch_geometric.loader.dataloader.DataLoader at 0x28c37ed90>,\n",
       " <torch_geometric.loader.dataloader.DataLoader at 0x17772abd0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_dataloader(), dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configuring a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a normal `torch.nn.Module`. The easiest way is to import one of the modules that we have already written in the `gnn_tracking` librar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphConstructionFCNN(in_dim=14, out_dim=8, depth=5, hidden_dim=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you can also write your own. Here is a very simple one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoGraphConstructionModel(nn.Module, HyperparametersMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden_dim: int,\n",
    "        out_dim: int,\n",
    "        depth: int = 5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # This is made available by the HyperparametersMixin\n",
    "        # all of our hyperparameters from the __init__ arguments\n",
    "        # are saved to self.hparams (but we don't need this in this\n",
    "        # example)\n",
    "        self.save_hyperparameters()\n",
    "        assert depth > 2\n",
    "        _layers = [\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "        for _ in range(depth - 2):\n",
    "            _layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            _layers.append(nn.ReLU())\n",
    "        _layers.append(nn.Linear(hidden_dim, out_dim))\n",
    "        self._model = nn.Sequential(*_layers)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        # Our trainer class will expect us to return a dictionary, where\n",
    "        # the key H has the transformed latent space.\n",
    "        return {\"H\": self._model(data.x)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DemoGraphConstructionModel(in_dim=14, out_dim=8, hidden_dim=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with normal pytorch, there was only few differences:\n",
    "\n",
    "1. We inherit from `HyperparamsMixin`\n",
    "2. We call `self.save_hyperparameters()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details (for understanding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saved all hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"beta\":       0.4\n",
       "\"depth\":      5\n",
       "\"hidden_dim\": 64\n",
       "\"in_dim\":     14\n",
       "\"out_dim\":    8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how `depth=5` was saved despite not being specified explicitly (it was recognized as a default parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, you can simply evaluate the `model` on a piece of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuring loss functions, metrics and the lightning module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pytorch model is bundled together with a set of loss functions (just one here), that we backpropagate from in the training step, and a set of metrics. Together, these components make up the `LightningModule` that we pass to the pytorch lightning `Trainer` for training.\n",
    "\n",
    "If you were familiar with our previous `TCNTrainer` training class, this `MLModule` now fulfills (almost) the exact same role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:06:49] DEBUG: Got obj of type <class 'gnn_tracking.models.graph_construction.GraphConstructionFCNN'>, assuming I have to save hyperparameters\u001b[0m\n",
      "\u001b[36m[10:06:49] DEBUG: Saving hyperperameters {'class_path': 'gnn_tracking.models.graph_construction.GraphConstructionFCNN', 'init_args': {'in_dim': 14, 'hidden_dim': 64, 'out_dim': 8, 'depth': 5, 'beta': 0.4}}\u001b[0m\n",
      "\u001b[36m[10:06:49] DEBUG: Got obj of type <class 'gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss'>, assuming I have to save hyperparameters\u001b[0m\n",
      "\u001b[36m[10:06:49] DEBUG: Saving hyperperameters {'class_path': 'gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss', 'init_args': {'r_emb': 0.002, 'max_num_neighbors': 10, 'attr_pt_thld': 0.9, 'p_attr': 1, 'p_rep': 1}}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "lmodel = MLModule(\n",
    "    model=model,\n",
    "    loss_fct=GraphConstructionHingeEmbeddingLoss(max_num_neighbors=10),\n",
    "    lw_repulsive=0.5,  # loss weight, see below\n",
    "    optimizer=partial(torch.optim.Adam, lr=1e-4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details (for understanding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, all hyperparameters are accessible (even the ones that weren't explicitly specified but only set by default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"loss_fct\":     {'class_path': 'gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss', 'init_args': {'r_emb': 0.002, 'max_num_neighbors': 10, 'attr_pt_thld': 0.9, 'p_attr': 1, 'p_rep': 1}}\n",
       "\"lw_repulsive\": 0.5\n",
       "\"model\":        {'class_path': 'gnn_tracking.models.graph_construction.GraphConstructionFCNN', 'init_args': {'in_dim': 14, 'hidden_dim': 64, 'out_dim': 8, 'depth': 5, 'beta': 0.4}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmodel.hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, any _objects_ that were passed to the model are also saved to the hyperparameters in a way that we can bring them back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function takes output from the model and the data and returns two separate losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attractive': tensor(0.0221, grad_fn=<DivBackward0>),\n",
       " 'repulsive': tensor(0.0020, grad_fn=<SumBackward0>)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fct = GraphConstructionHingeEmbeddingLoss()\n",
    "loss_fct(\n",
    "    x=out[\"H\"],\n",
    "    particle_id=data.particle_id,\n",
    "    batch=data.batch,\n",
    "    edge_index=data.edge_index,\n",
    "    pt=data.pt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both parts of the loss functions are combined with the loss weight we have configured above (weight of 1 for attractive, weight of 0.5 for repulsive). All of this is done in `MLModule.get_losses` (returning the total loss and a dictionary of the individual losses):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0270, grad_fn=<AddBackward0>),\n",
       " {'attractive': 0.02667856030166149,\n",
       "  'repulsive': 0.0006350235780701041,\n",
       "  'attractive_weighted': 0.02667856030166149,\n",
       "  'repulsive_weighted': 0.00031751178903505206,\n",
       "  'total': 0.026996072381734848})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmodel.get_losses(out, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m[10:06:53] INFO: DataLoader will load 5 graphs (out of 90 available).\u001b[0m\n",
      "\u001b[36m[10:06:53] DEBUG: First graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21025_s0.pt, last graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21053_s0.pt\u001b[0m\n",
      "\u001b[32m[10:06:53] INFO: DataLoader will load 5 graphs (out of 90 available).\u001b[0m\n",
      "\u001b[36m[10:06:53] DEBUG: First graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21058_s0.pt, last graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21094_s0.pt\u001b[0m\n",
      "\n",
      "  | Name     | Type                                | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | model    | GraphConstructionFCNN               | 17.8 K\n",
      "1 | loss_fct | GraphConstructionHingeEmbeddingLoss | 0     \n",
      "-----------------------------------------------------------------\n",
      "17.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "17.8 K    Total params\n",
      "0.071     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c907a1f6cc49b68e6c5a5ac3f3203b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a9030cd2654e7fbf9beb91768e49af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fe756a665d4220a023c2a7b53726f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[3m            Validation epoch=1             \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mMetric             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Value\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Error\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━┩\n",
      "│ attractive          │ 0.02582 │ 0.00079 │\n",
      "│ attractive_weighted │ 0.02582 │ 0.00079 │\n",
      "│ repulsive           │ 0.00068 │ 0.00002 │\n",
      "│ repulsive_weighted  │ 0.00034 │ 0.00001 │\n",
      "│ total               │ 0.02616 │ 0.00079 │\n",
      "└─────────────────────┴─────────┴─────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=1, accelerator=\"cpu\", log_every_n_steps=1)\n",
    "trainer.fit(model=lmodel, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### If there are issues with the progress bar\n",
    "\n",
    "The lightning progress bar can be finnicky when combined with printing the validation results to the command line, especially when running from a Jupyter notebook. Here's a couple of things to try:\n",
    "\n",
    "* set `enable_progress_bar=False` in the `Trainer` initialization to disable the progress bar\n",
    "* use `callbacks=[pytorch_lightning.callbacks.RichProgressBar(leave=True)]` in the `Trainer` initialization (this is a prettier progress bar, anyway). I\n",
    "* use `callbacks=[gnn_tracking.utils.lightning.SimpleTqdmProgressBar(leave=True)]`\n",
    "* set `lmodel.print_validation_results=False` to disable printing the validation results to the command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restoring a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the `lightning_logs` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls lightning_logs/version_0/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to one of the versions and take a look at the `hparams.yaml` file. It should contain exactly the hyperparameters from the run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat lightning_logs/version_0/hparams.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can bring back the trained model by loading one of the checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_model = MLModule.load_from_checkpoint(\n",
    "    \"lightning_logs/version_0/checkpoints/epoch=0-step=5.ckpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we didn't have to specify any the hyperparameters again.\n",
    "\n",
    "However, we can easily change some of them by adding them as additional keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_model_modified = MLModule.load_from_checkpoint(\n",
    "    \"lightning_logs/version_0/checkpoints/epoch=0-step=5.ckpt\",\n",
    "    lw_repulsive=0.1,\n",
    "    loss_fct=GraphConstructionHingeEmbeddingLoss(max_num_neighbors=5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you cannot modify the model architecture however (but you could in principle change the `beta` parameter of the residual connections)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running all of this from the command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the following can be achieved by running the following command:\n",
    "\n",
    "```bash\n",
    "python3 gnn_tracking/trainers/run.py fit --model configs/model.yml --data configs/data.yml  --trainer.accelerator cpu --trainer.accelerator cpu\n",
    "```\n",
    "\n",
    "with the data config file\n",
    "\n",
    "```yaml\n",
    "train:\n",
    "  dirs:\n",
    "    - /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/\n",
    "  stop: 5\n",
    "test:\n",
    "  dirs:\n",
    "    - /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/\n",
    "  star: 10\n",
    "  stop: 15\n",
    "val:\n",
    "  dirs:\n",
    "    - /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/\n",
    "  start: 5\n",
    "  stop: 10\n",
    "```\n",
    "\n",
    "and model config file:\n",
    "\n",
    "```\n",
    "class_path: gnn_tracking.training.ml.MLModule\n",
    "init_args:\n",
    "  model:\n",
    "    class_path: gnn_tracking.models.graph_construction.GraphConstructionFCNN\n",
    "    init_args:\n",
    "      in_dim: 14\n",
    "      out_dim: 8\n",
    "      hidden_dim: 512\n",
    "      depth: 5\n",
    "  lw_repulsive: 0.5\n",
    "  loss_fct:\n",
    "    class_path: gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss\n",
    "    init_args: {}\n",
    "  optimizer:\n",
    "    class_path: torch.optim.Adam\n",
    "    init_args:\n",
    "      lr: 0.0001\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quickly override one of the options, you can simply add them to the command line, e.g., `--model.init_args.lw_repulsive=0.1` or `--model.model.init_args.depth=6`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
