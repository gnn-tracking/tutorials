{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Building graphs with Metric Learning\n",
    "\n",
    "This notebook shows how to build graphs using a metric learning strategy. For this, every hit is independently projected to a latent space using a fully connected neural network. The network is trained to put hits from the same particle close to each other and hits from different particles far from each other. An initial graph can then be constructed by connecting hits that are close in this space.\n",
    "This strategy has been adapted by ExaTrkx, see for example [section 5.2 here.](https://link.springer.com/10.1140/epjc/s10052-021-09675-8)\n",
    "\n",
    "This notebook also serves as an introduction to the new pytorch lightning-based framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from gnn_tracking.training.ml import MLModule\n",
    "from gnn_tracking.models.graph_construction import GraphConstructionFCNN\n",
    "from gnn_tracking.metrics.losses.metric_learning import GraphConstructionHingeEmbeddingLoss\n",
    "from pytorch_lightning import Trainer\n",
    "from gnn_tracking.utils.loading import TrackingDataModule\n",
    "from gnn_tracking.training.callbacks import PrintValidationMetrics\n",
    "from gnn_tracking.utils.versioning import assert_version_geq\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch import nn\n",
    "from pytorch_lightning.core.mixins import HyperparametersMixin\n",
    "\n",
    "assert_version_geq(\"23.12.0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuring the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration for train/val/test data and its dataloader is held in the `TrackingDataModule` (subclass of `LightningDataModule`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = Path.cwd().resolve().parent.parent / \"test-data\" / \"data\" / \"point_clouds\" / \"v8\"\n",
    "assert data_dir.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dm = TrackingDataModule(\n",
    "    train=dict(\n",
    "        dirs=[data_dir],\n",
    "        stop=1,\n",
    "    ),\n",
    "    val=dict(\n",
    "        dirs=[data_dir],\n",
    "        start=1,\n",
    "        stop=2,\n",
    "    ),\n",
    "    identifier=\"point_clouds_v8\"\n",
    "    # could also configure a 'test' set here\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other keys allow to configure the loaders (batch size, number of workers, etc.). See the docstring of `TrackingDataModule` for details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details (for understanding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all of the following will be done implicitly by the `Trainer` and you won't have to worry about it. But if you want to inspect the data, you can do so.\n",
    "\n",
    "When calling the `setup` method, the `LightningDataModule` initializes instances of `TrackingDataset` (`torch_geometric.Dataset`) for each of these. We can get the corresponding dataloaders by calling `dm.train_dataloader()` and analog for validation and test.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[14:44:10] INFO: DataLoader will load 1 graphs (out of 2 available).\u001b[0m\n",
      "\u001b[36m[14:44:10] DEBUG: First graph is /home/kl5675/Documents/23/git_sync/test-data/data/point_clouds/v8/data21000_s0.pt, last graph is /home/kl5675/Documents/23/git_sync/test-data/data/point_clouds/v8/data21000_s0.pt\u001b[0m\n",
      "\u001b[32m[14:44:10] INFO: DataLoader will load 1 graphs (out of 2 available).\u001b[0m\n",
      "\u001b[36m[14:44:10] DEBUG: First graph is /home/kl5675/Documents/23/git_sync/test-data/data/point_clouds/v8/data21001_s0.pt, last graph is /home/kl5675/Documents/23/git_sync/test-data/data/point_clouds/v8/data21001_s0.pt\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': TrackingDataset(), 'val': TrackingDataset()}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is called by the Trainer automatically and sets up the datasets\n",
    "dm.setup(stage=\"fit\")  # 'fit' combines 'train' and 'val'\n",
    "# Now the datasets are available:\n",
    "dm.datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can inspect the first element of the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "data = dm.datasets[\"train\"][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the corresponding dataloaders, use one of the methods (but again, you probalby won't need to):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch_geometric.loader.dataloader.DataLoader at 0x14a2b23bd1b0>,\n",
       " <torch_geometric.loader.dataloader.DataLoader at 0x14a3a23c4160>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_dataloader(), dm.val_dataloader()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configuring a model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a normal `torch.nn.Module`. The easiest way is to import one of the modules that we have already written in the `gnn_tracking` librar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = GraphConstructionFCNN(in_dim=14, out_dim=8, depth=5, hidden_dim=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you can also write your own. Here is a very simple one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class DemoGraphConstructionModel(nn.Module, HyperparametersMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden_dim: int,\n",
    "        out_dim: int,\n",
    "        depth: int = 5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # This is made available by the HyperparametersMixin\n",
    "        # all of our hyperparameters from the __init__ arguments\n",
    "        # are saved to self.hparams (but we don't need this in this\n",
    "        # example)\n",
    "        self.save_hyperparameters()\n",
    "        assert depth > 2\n",
    "        _layers = [\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "        for _ in range(depth - 2):\n",
    "            _layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            _layers.append(nn.ReLU())\n",
    "        _layers.append(nn.Linear(hidden_dim, out_dim))\n",
    "        self._model = nn.Sequential(*_layers)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        # Our trainer class will expect us to return a dictionary, where\n",
    "        # the key H has the transformed latent space.\n",
    "        return {\"H\": self._model(data.x)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the next line to use model we just wrote (rather than the default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# model = DemoGraphConstructionModel(in_dim=14, out_dim=8, hidden_dim=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with normal pytorch, there was only few differences:\n",
    "\n",
    "1. We inherit from `HyperparamsMixin`\n",
    "2. We call `self.save_hyperparameters()`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details (for understanding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saved all hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"alpha\":      0.6\n",
       "\"depth\":      5\n",
       "\"hidden_dim\": 64\n",
       "\"in_dim\":     14\n",
       "\"out_dim\":    8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how `depth=5` was saved despite not being specified explicitly (it was recognized as a default parameter)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, you can simply evaluate the `model` on a piece of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "out = model(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuring loss functions, metrics and the lightning module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pytorch model is bundled together with a set of loss functions (just one here), that we backpropagate from in the training step, and a set of metrics. Together, these components make up the `LightningModule` that we pass to the pytorch lightning `Trainer` for training.\n",
    "\n",
    "If you were familiar with our previous `TCNTrainer` training class, this `MLModule` now fulfills (almost) the exact same role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "lmodel = MLModule(\n",
    "    model=model,\n",
    "    loss_fct=GraphConstructionHingeEmbeddingLoss(\n",
    "        lw_repulsive=0.5,\n",
    "        max_num_neighbors=10,\n",
    "    ),\n",
    "    optimizer=partial(torch.optim.Adam, lr=1e-4),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details (for understanding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, all hyperparameters are accessible (even the ones that weren't explicitly specified but only set by default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"gc_scanner\": None\n",
       "\"loss_fct\":   {'class_path': 'gnn_tracking.metrics.losses.metric_learning.GraphConstructionHingeEmbeddingLoss', 'init_args': {'lw_repulsive': 0.5, 'r_emb': 1.0, 'max_num_neighbors': 10, 'pt_thld': 0.9, 'max_eta': 4.0, 'p_attr': 1.0, 'p_rep': 1.0}}\n",
       "\"model\":      {'class_path': 'gnn_tracking.models.graph_construction.GraphConstructionFCNN', 'init_args': {'in_dim': 14, 'hidden_dim': 64, 'out_dim': 8, 'depth': 5, 'alpha': 0.6}}\n",
       "\"preproc\":    None"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmodel.hparams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, any _objects_ that were passed to the model are also saved to the hyperparameters in a way that we can bring them back."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function takes output from the model and the data and returns two separate losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[66114, 14], edge_index=[2, 229066], y=[0], layer=[66114], particle_id=[66114], pt=[66114], reconstructable=[66114], sector=[66114], eta=[66114], n_hits=[66114], n_layers_hit=[66114])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLossFctReturn(loss_dct={'attractive': tensor(0.0303, grad_fn=<DivBackward1>), 'repulsive': tensor(0.9888, grad_fn=<DifferentiableGraphBackward>)}, weight_dct={'attractive': 1.0, 'repulsive': 1.0}, extra_metrics={})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fct = GraphConstructionHingeEmbeddingLoss()\n",
    "loss_fct(\n",
    "    x=out[\"H\"],\n",
    "    particle_id=data.particle_id,\n",
    "    batch=data.batch,\n",
    "    edge_index=data.edge_index,\n",
    "    pt=data.pt,\n",
    "    eta=data.eta,\n",
    "    reconstructable=data.reconstructable,\n",
    "    true_edge_index=data.edge_index,\n",
    "    max_num_neighbors=2,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both parts of the loss functions are combined with the loss weight we have configured above (weight of 1 for attractive, weight of 0.5 for repulsive). All of this is done in `MLModule.get_losses` (returning the total loss and a dictionary of the individual losses):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5280, grad_fn=<AddBackward0>),\n",
       " {'attractive': tensor(0.0303, grad_fn=<DivBackward1>),\n",
       "  'repulsive': tensor(0.9954, grad_fn=<DifferentiableGraphBackward>),\n",
       "  'attractive_weighted': 0.03026364929974079,\n",
       "  'repulsive_weighted': 0.4976946711540222,\n",
       "  'total': 0.527958333492279})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmodel.get_losses(out, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[14:56:36] INFO: DataLoader will load 1 graphs (out of 2 available).\u001b[0m\n",
      "\u001b[36m[14:56:36] DEBUG: First graph is /home/kl5675/Documents/23/git_sync/test-data/data/point_clouds/v8/data21000_s0.pt, last graph is /home/kl5675/Documents/23/git_sync/test-data/data/point_clouds/v8/data21000_s0.pt\u001b[0m\n",
      "\u001b[32m[14:56:36] INFO: DataLoader will load 1 graphs (out of 2 available).\u001b[0m\n",
      "\u001b[36m[14:56:36] DEBUG: First graph is /home/kl5675/Documents/23/git_sync/test-data/data/point_clouds/v8/data21001_s0.pt, last graph is /home/kl5675/Documents/23/git_sync/test-data/data/point_clouds/v8/data21001_s0.pt\u001b[0m\n",
      "\n",
      "  | Name     | Type                                | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | model    | GraphConstructionFCNN               | 17.8 K\n",
      "1 | loss_fct | GraphConstructionHingeEmbeddingLoss | 0     \n",
      "-----------------------------------------------------------------\n",
      "17.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "17.8 K    Total params\n",
      "0.071     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|███████████████████████████████████████████████████████████| 1/1 [01:38<00:00,  0.01it/s, v_num=2, attractive_train=0.0301, repulsive_train=0.995, attractive_weighted_train=0.0301, repulsive_weighted_train=0.498, total_train=0.528]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m              Validation epoch=0               \u001b[0m                                                                                                                                                                                              \n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mMetric                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Value\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mError\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━┩\n",
      "│\u001b[1;95m \u001b[0m\u001b[1;95mattractive               \u001b[0m\u001b[1;95m \u001b[0m│\u001b[1;95m \u001b[0m\u001b[1;95m0.03383\u001b[0m\u001b[1;95m \u001b[0m│\u001b[1;95m \u001b[0m\u001b[1;95m  nan\u001b[0m\u001b[1;95m \u001b[0m│\n",
      "│ attractive_train          │ 0.03009 │   nan │\n",
      "│ attractive_weighted       │ 0.03383 │   nan │\n",
      "│ attractive_weighted_train │ 0.03009 │   nan │\n",
      "│\u001b[1;95m \u001b[0m\u001b[1;95mrepulsive                \u001b[0m\u001b[1;95m \u001b[0m│\u001b[1;95m \u001b[0m\u001b[1;95m0.99553\u001b[0m\u001b[1;95m \u001b[0m│\u001b[1;95m \u001b[0m\u001b[1;95m  nan\u001b[0m\u001b[1;95m \u001b[0m│\n",
      "│ repulsive_train           │ 0.99535 │   nan │\n",
      "│ repulsive_weighted        │ 0.49777 │   nan │\n",
      "│ repulsive_weighted_train  │ 0.49767 │   nan │\n",
      "│\u001b[1;95m \u001b[0m\u001b[1;95mtotal                    \u001b[0m\u001b[1;95m \u001b[0m│\u001b[1;95m \u001b[0m\u001b[1;95m0.53160\u001b[0m\u001b[1;95m \u001b[0m│\u001b[1;95m \u001b[0m\u001b[1;95m  nan\u001b[0m\u001b[1;95m \u001b[0m│\n",
      "│ total_train               │ 0.52777 │   nan │\n",
      "└───────────────────────────┴─────────┴───────┘\n",
      "\n",
      "Epoch 0: 100%|███████████████████████████████████████████████████████████| 1/1 [03:02<00:00,  0.01it/s, v_num=2, attractive_train=0.0301, repulsive_train=0.995, attractive_weighted_train=0.0301, repulsive_weighted_train=0.498, total_train=0.528]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|███████████████████████████████████████████████████████████| 1/1 [03:02<00:00,  0.01it/s, v_num=2, attractive_train=0.0301, repulsive_train=0.995, attractive_weighted_train=0.0301, repulsive_weighted_train=0.498, total_train=0.528]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"cpu\",\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[PrintValidationMetrics()],\n",
    ")\n",
    "trainer.fit(model=lmodel, datamodule=dm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### If there are issues with the progress bar\n",
    "\n",
    "The lightning progress bar can be finnicky when combined with printing the validation results to the command line, especially when running from a Jupyter notebook. Here's a couple of things to try:\n",
    "\n",
    "* set `enable_progress_bar=False` in the `Trainer` initialization to disable the progress bar\n",
    "* use `callbacks=[pytorch_lightning.callbacks.RichProgressBar(leave=True), ...]` in the `Trainer` initialization (this is a prettier progress bar, anyway). I\n",
    "* use `callbacks=[gnn_tracking.utils.lightning.SimpleTqdmProgressBar(leave=True), ...]`\n",
    "* remove the `PrintValidationMetrics` callback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restoring a pre-trained model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the `lightning_logs` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version_0  version_1  version_2\n"
     ]
    }
   ],
   "source": [
    "! ls lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the latest version number in the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'epoch=0-step=1.ckpt'\n"
     ]
    }
   ],
   "source": [
    "! ls lightning_logs/version_2/checkpoints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to one of the versions and take a look at the `hparams.yaml` file. It should contain exactly the hyperparameters from the run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n",
      "  class_path: gnn_tracking.models.graph_construction.GraphConstructionFCNN\n",
      "  init_args:\n",
      "    in_dim: 14\n",
      "    hidden_dim: 64\n",
      "    out_dim: 8\n",
      "    depth: 5\n",
      "    alpha: 0.6\n",
      "preproc: null\n",
      "loss_fct:\n",
      "  class_path: gnn_tracking.metrics.losses.metric_learning.GraphConstructionHingeEmbeddingLoss\n",
      "  init_args:\n",
      "    lw_repulsive: 0.5\n",
      "    r_emb: 1.0\n",
      "    max_num_neighbors: 10\n",
      "    pt_thld: 0.9\n",
      "    max_eta: 4.0\n",
      "    p_attr: 1.0\n",
      "    p_rep: 1.0\n",
      "gc_scanner: null\n"
     ]
    }
   ],
   "source": [
    "! cat lightning_logs/version_2/hparams.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can check out the `config.yaml` file for additional config values affecting the `Trainer` and other elements of the training process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can bring back the trained model by loading one of the checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[15:03:04] DEBUG: Getting class GraphConstructionFCNN from module gnn_tracking.models.graph_construction\u001b[0m\n",
      "\u001b[36m[15:03:04] DEBUG: Getting class GraphConstructionHingeEmbeddingLoss from module gnn_tracking.metrics.losses.metric_learning\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "restored_model = MLModule.load_from_checkpoint(\n",
    "    \"lightning_logs/version_2/checkpoints/epoch=0-step=1.ckpt\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we didn't have to specify any the hyperparameters again.\n",
    "\n",
    "However, we can easily change some of them by adding them as additional keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[15:03:23] DEBUG: Getting class GraphConstructionFCNN from module gnn_tracking.models.graph_construction\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "restored_model_modified = MLModule.load_from_checkpoint(\n",
    "    \"lightning_logs/version_2/checkpoints/epoch=0-step=1.ckpt\",\n",
    "    loss_fct=GraphConstructionHingeEmbeddingLoss(lw_repulsive=0.1, max_num_neighbors=5),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you cannot modify the model architecture however (but you could in principle change the `beta` parameter of the residual connections)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running all of this from the command line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the following can be achieved by running the following command:\n",
    "\n",
    "```bash\n",
    "python3 gnn_tracking/trainers/run.py fit --model configs/model.yml --data configs/data.yml  --trainer.accelerator cpu --trainer.accelerator cpu\n",
    "```\n",
    "\n",
    "with the data config file\n",
    "\n",
    "```yaml\n",
    "train:\n",
    "  dirs:\n",
    "    - /path/to/your/dir\n",
    "  stop: 5\n",
    "test:\n",
    "  dirs:\n",
    "    - /path/to/your/dir\n",
    "  star: 10\n",
    "  stop: 15\n",
    "val:\n",
    "  dirs:\n",
    "    - /path/to/your/dir\n",
    "  start: 5\n",
    "  stop: 10\n",
    "identifier: point_clouds_v8\n",
    "```\n",
    "\n",
    "and model config file:\n",
    "\n",
    "```yaml\n",
    "class_path: gnn_tracking.training.ml.MLModule\n",
    "init_args:\n",
    "  model:\n",
    "    class_path: gnn_tracking.models.graph_construction.GraphConstructionFCNN\n",
    "    init_args:\n",
    "      in_dim: 14\n",
    "      out_dim: 8\n",
    "      hidden_dim: 512\n",
    "      depth: 5\n",
    "  loss_fct:\n",
    "    class_path: gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss\n",
    "    init_args:\n",
    "      lw_repulsive: 0.5\n",
    "  optimizer:\n",
    "    class_path: torch.optim.Adam\n",
    "    init_args:\n",
    "      lr: 0.0001\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quickly override one of the options, you can simply add them to the command line, e.g., `--model.init_args.loss_fct.init_args.lw_repulsive=0.1` or `--model.model.init_args.depth=6`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Connecting with Weights & Biases\n",
    "\n",
    "Weights and Biases (wandb.ai) is a great tool to log all of your runs to. \n",
    "It's also very easy to set up in principle by adding a callback to the `Trainer`.\n",
    "\n",
    "However, first you need to create an account (it's free!). If you collaborate with us, you probably want to reach out to us so that we can add you to our project (and can see each other's runs).\n",
    "\n",
    "Once you have your account, copy your API key into the file `~/.wandb_api_key` on the server from which you run your ML models.\n",
    "\n",
    "Because we want to later identify our current trial among other trials (and have an easy-to-remember name), let's first create an identifier:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from gnn_tracking.utils.nomenclature import random_trial_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────── </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">demonic-logical-platypus</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────── \u001b[0m\u001b[1;33mdemonic-logical-platypus\u001b[0m\u001b[92m ───────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name = random_trial_name()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After this, let's set up the logger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id godlike-buzzard-of-wonder.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"ml\",\n",
    "    group=\"first\",\n",
    "    offline=True,  # <-- see notes below\n",
    "    version=name,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to keep our checkpoints locally, so let's also initialize the default logger (which would be replaced by `WandbLogger` if we don't add it manually):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "tb_logger = TensorBoardLogger(\".\", version=name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'd have all the places in place, if it weren't for one subtlety: The Princeton compute nodes don't have internet connectivity.\n",
    "This is also why we set `offline=True` to the `WandbLogger`. But that's not a problem, because we have internet on the head node `della-gpu`\n",
    "(just not on the compute node). So we can simply run `wandb sync /path/to/run/dir` afterwards.\n",
    "However, because this is annoying, I wrote a package `wandb-osh` to help with this.\n",
    "\n",
    "To install it, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb-osh in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (1.0.4)\n",
      "Requirement already satisfied: colorlog in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from wandb-osh) (6.7.0)\n",
      "Requirement already satisfied: wandb in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from wandb-osh) (0.15.4)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from wandb->wandb-osh) (8.1.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from wandb->wandb-osh) (3.1.31)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from wandb->wandb-osh) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from wandb->wandb-osh) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from wandb->wandb-osh) (1.21.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from wandb->wandb-osh) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from wandb->wandb-osh) (6.0)\n",
      "Requirement already satisfied: pathtools in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from wandb->wandb-osh) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from wandb->wandb-osh) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from wandb->wandb-osh) (67.7.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from wandb->wandb-osh) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from wandb->wandb-osh) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb->wandb-osh) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->wandb-osh) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb->wandb-osh) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb->wandb-osh) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb->wandb-osh) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb->wandb-osh) (2023.5.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->wandb-osh) (3.0.5)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install wandb-osh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put everything together: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from wandb_osh.lightning_hooks import TriggerWandbSyncLightningCallback\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator=\"cpu\",\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[\n",
    "        TriggerWandbSyncLightningCallback(),\n",
    "        PrintValidationMetrics(),\n",
    "    ],\n",
    "    logger=[\n",
    "        wandb_logger,\n",
    "        tb_logger,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sync your run, simply start the `wandb-osh` command line utility on `della-gpu`.\n",
    "For more information on how this works, see [here](https://github.com/klieret/wandb-offline-sync-hook)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
