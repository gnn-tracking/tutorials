{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Building graphs with Metric Learning (and an introduction to the new pytorch lightning-based framework)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "\n",
    "from gnn_tracking.training.ml import MLModule\n",
    "from gnn_tracking.models.graph_construction import GraphConstructionFCNN\n",
    "from gnn_tracking.metrics.losses import GraphConstructionHingeEmbeddingLoss\n",
    "from pytorch_lightning import Trainer\n",
    "from gnn_tracking.utils.loading import TrackingDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Configuring the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The configuration for train/val/test data and its dataloader is held in a `LightningDataModule`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "dm = TrackingDataModule(\n",
    "    train=dict(\n",
    "        dirs=[\"/Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/\"],\n",
    "        stop=5,\n",
    "    ),\n",
    "    val=dict(\n",
    "        dirs=[\"/Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/\"],\n",
    "        start=5,\n",
    "        stop=10,\n",
    "    ),\n",
    "    # could also configure a 'test' set here\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Other keys allow to configure the loaders (batch size, number of workers, etc.). See the docstring of `TrackingDataModule` for details."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Details (for understanding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that all of the following will be done implicitly by the `Trainer` and you won't have to worry about it. But if you want to inspect the data, you can do so.\n",
    "\n",
    "When calling the `setup` method, the `LightningDataModule` initializes instances of `TrackingDataset` (`torch_geometric.Dataset`) for each of these. We can get the corresponding dataloaders by calling `dm.train_dataloader()` and analog for validation and test.\n",
    "\n",
    "Example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[19:29:55] INFO: DataLoader will load 5 graphs (out of 90 available).\u001b[0m\n",
      "\u001b[36m[19:29:55] DEBUG: First graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21025_s0.pt, last graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21053_s0.pt\u001b[0m\n",
      "\u001b[32m[19:29:55] INFO: DataLoader will load 5 graphs (out of 90 available).\u001b[0m\n",
      "\u001b[36m[19:29:55] DEBUG: First graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21058_s0.pt, last graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21094_s0.pt\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'train': TrackingDataset(5), 'val': TrackingDataset(5)}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is called by the Trainer automatically and sets up the datasets\n",
    "dm.setup(stage=\"fit\")  # 'fit' combines 'train' and 'val'\n",
    "# Now the datasets are available:\n",
    "dm.datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For example, we can inspect the first element of the training dataset:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "data = dm.datasets[\"train\"][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get the corresponding dataloaders, use one of the methods (but again, you probalby won't need to):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(<torch_geometric.loader.dataloader.DataLoader at 0x147d2f090>,\n <torch_geometric.loader.dataloader.DataLoader at 0x15815d950>)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_dataloader(), dm.val_dataloader()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Configuring a model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We write a normal `torch.nn.Module`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model = GraphConstructionFCNN(in_dim=14, out_dim=8, depth=5, hidden_dim=64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Details (for understanding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The only difference is a call `self.save_hyperparameters()` in `__init__` (and the inheritance from `HyperparametersMixin`). This allows us to save all hyperparameters:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "\"beta\":       0.4\n\"depth\":      5\n\"hidden_dim\": 64\n\"in_dim\":     14\n\"out_dim\":    8"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note how `beta=0.4` was saved despite not being specified explicitly (it's set as a default parameter)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As always, you can simply evaluate the `model` on a piece of data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "out = model(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Configuring loss functions and metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The pytorch model is bundled together with a set of loss functions (just one here), that we backpropagate from in the training step, and a set of metrics. Together, these components make up the `LightningModule` that we pass to the pytorch lightning `Trainer` for training.\n",
    "\n",
    "If you were familiar with our previous `TCNTrainer` training class, this `MLModule` now fulfills (almost) the exact same role."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[19:25:53] DEBUG: Got obj of type <class 'gnn_tracking.models.graph_construction.GraphConstructionFCNN'>, assuming I have to save hyperparameters\u001b[0m\n",
      "\u001b[36m[19:25:53] DEBUG: Saving hyperperameters {'class_path': 'gnn_tracking.models.graph_construction.GraphConstructionFCNN', 'init_args': {'in_dim': 14, 'hidden_dim': 64, 'out_dim': 8, 'depth': 5, 'beta': 0.4}}\u001b[0m\n",
      "\u001b[36m[19:25:53] DEBUG: Got obj of type <class 'gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss'>, assuming I have to save hyperparameters\u001b[0m\n",
      "\u001b[36m[19:25:53] DEBUG: Saving hyperperameters {'class_path': 'gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss', 'init_args': {'r_emb': 0.002, 'max_num_neighbors': 10, 'attr_pt_thld': 0.9, 'p_attr': 1, 'p_rep': 1}}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "lmodel = MLModule(\n",
    "    model=model,\n",
    "    loss_fct=GraphConstructionHingeEmbeddingLoss(max_num_neighbors=10),\n",
    "    lw_repulsive=0.5,  # loss weight, see below\n",
    "    optimizer=partial(torch.optim.Adam, lr=1e-4),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Details (for understanding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, all hyperparameters are accessible (even the ones that weren't explicitly specified but only set by default):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "\"loss_fct\":     {'class_path': 'gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss', 'init_args': {'r_emb': 0.002, 'max_num_neighbors': 10, 'attr_pt_thld': 0.9, 'p_attr': 1, 'p_rep': 1}}\n\"lw_repulsive\": 0.5\n\"model\":        {'class_path': 'gnn_tracking.models.graph_construction.GraphConstructionFCNN', 'init_args': {'in_dim': 14, 'hidden_dim': 64, 'out_dim': 8, 'depth': 5, 'beta': 0.4}}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmodel.hparams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The loss function takes output from the model and the data and returns two separate losses:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "{'attractive': tensor(0.0257, grad_fn=<DivBackward0>),\n 'repulsive': tensor(0.0028, grad_fn=<SumBackward0>)}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fct = GraphConstructionHingeEmbeddingLoss()\n",
    "loss_fct(\n",
    "    x=out[\"H\"],\n",
    "    particle_id=data.particle_id,\n",
    "    batch=data.batch,\n",
    "    edge_index=data.edge_index,\n",
    "    pt=data.pt,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Both parts of the loss functions are combined with the loss weight we have configured above (weight of 1 for attractive, weight of 0.5 for repulsive). All of this is done in `MLModule.get_losses` (returning the total loss and a dictionary of the individual losses):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(0.0318, grad_fn=<AddBackward0>),\n {'attractive': 0.03146896883845329,\n  'repulsive': 0.0006920472951605916,\n  'attractive_weighted': 0.03146896883845329,\n  'repulsive_weighted': 0.0003460236475802958})"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmodel.get_losses(out, data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: /Users/fuchur/Documents/22/git_sync/gnn_tracking/lightning_logs\n",
      "\u001b[32m[19:37:33] INFO: DataLoader will load 5 graphs (out of 90 available).\u001b[0m\n",
      "\u001b[36m[19:37:33] DEBUG: First graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21025_s0.pt, last graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21053_s0.pt\u001b[0m\n",
      "\u001b[32m[19:37:33] INFO: DataLoader will load 5 graphs (out of 90 available).\u001b[0m\n",
      "\u001b[36m[19:37:33] DEBUG: First graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21058_s0.pt, last graph is /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/data21094_s0.pt\u001b[0m\n",
      "\n",
      "  | Name     | Type                                | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | model    | GraphConstructionFCNN               | 17.8 K\n",
      "1 | loss_fct | GraphConstructionHingeEmbeddingLoss | 0     \n",
      "-----------------------------------------------------------------\n",
      "17.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "17.8 K    Total params\n",
      "0.071     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8795eba9bf44dd49895287d72b13b72"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 6789. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8459. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "\n\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001b[3m           Validation epoch=0            \u001b[0m\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mMetric             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Value\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mError\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━┩\n│ attractive          │ 0.03392 │   nan │\n│ attractive_weighted │ 0.03392 │   nan │\n│ repulsive           │ 0.00078 │   nan │\n│ repulsive_weighted  │ 0.00039 │   nan │\n│ total               │ 0.03431 │   nan │\n└─────────────────────┴─────────┴───────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">           Validation epoch=0            </span>\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Metric              </span>┃<span style=\"font-weight: bold\">   Value </span>┃<span style=\"font-weight: bold\"> Error </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━┩\n│ attractive          │ 0.03392 │   nan │\n│ attractive_weighted │ 0.03392 │   nan │\n│ repulsive           │ 0.00078 │   nan │\n│ repulsive_weighted  │ 0.00039 │   nan │\n│ total               │ 0.03431 │   nan │\n└─────────────────────┴─────────┴───────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5f6e0991d8a41f3acee956d8b3d3b69"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32ff3ccb640c4dc283101299f33f7b14"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 7299. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 5946. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/Users/fuchur/micromamba/envs/gnn/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8207. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "\n\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001b[3m           Validation epoch=0            \u001b[0m\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mMetric             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Value\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mError\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━┩\n│ attractive          │ 0.03069 │   nan │\n│ attractive_weighted │ 0.03069 │   nan │\n│ repulsive           │ 0.00077 │   nan │\n│ repulsive_weighted  │ 0.00038 │   nan │\n│ total               │ 0.03107 │   nan │\n└─────────────────────┴─────────┴───────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">           Validation epoch=0            </span>\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Metric              </span>┃<span style=\"font-weight: bold\">   Value </span>┃<span style=\"font-weight: bold\"> Error </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━┩\n│ attractive          │ 0.03069 │   nan │\n│ attractive_weighted │ 0.03069 │   nan │\n│ repulsive           │ 0.00077 │   nan │\n│ repulsive_weighted  │ 0.00038 │   nan │\n│ total               │ 0.03107 │   nan │\n└─────────────────────┴─────────┴───────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=1, accelerator=\"cpu\", log_every_n_steps=1)\n",
    "trainer.fit(model=lmodel, train_dataloaders=dm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Restoring a pre-trained model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take a look at the `lightning_logs` directory:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mversion_0\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mversion_0\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "! ls lightning_logs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0-step=5.ckpt\r\n"
     ]
    }
   ],
   "source": [
    "! ls lightning_logs/version_0/checkpoints"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Navigate to one of the versions and take a look at the `hparams.yaml` file. It should contain exactly the hyperparameters from the run.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\r\n",
      "  class_path: gnn_tracking.models.graph_construction.GraphConstructionFCNN\r\n",
      "  init_args:\r\n",
      "    in_dim: 14\r\n",
      "    hidden_dim: 64\r\n",
      "    out_dim: 8\r\n",
      "    depth: 5\r\n",
      "    beta: 0.4\r\n",
      "lw_repulsive: 0.5\r\n",
      "loss_fct:\r\n",
      "  class_path: gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss\r\n",
      "  init_args:\r\n",
      "    r_emb: 0.002\r\n",
      "    max_num_neighbors: 10\r\n",
      "    attr_pt_thld: 0.9\r\n",
      "    p_attr: 1\r\n",
      "    p_rep: 1\r\n"
     ]
    }
   ],
   "source": [
    "! cat lightning_logs/version_0/hparams.yaml"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can bring back the trained model by loading one of the checkpoints:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[19:39:17] DEBUG: Got {'class_path': 'gnn_tracking.models.graph_construction.GraphConstructionFCNN', 'init_args': {'in_dim': 14, 'hidden_dim': 64, 'out_dim': 8, 'depth': 5, 'beta': 0.4}}, assuming I have to load\u001b[0m\n",
      "\u001b[36m[19:39:17] DEBUG: Getting class GraphConstructionFCNN from module gnn_tracking.models.graph_construction\u001b[0m\n",
      "\u001b[36m[19:39:17] DEBUG: Got {'class_path': 'gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss', 'init_args': {'r_emb': 0.002, 'max_num_neighbors': 10, 'attr_pt_thld': 0.9, 'p_attr': 1, 'p_rep': 1}}, assuming I have to load\u001b[0m\n",
      "\u001b[36m[19:39:17] DEBUG: Getting class GraphConstructionHingeEmbeddingLoss from module gnn_tracking.metrics.losses\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "restored_model = MLModule.load_from_checkpoint(\n",
    "    \"lightning_logs/version_0/checkpoints/epoch=0-step=5.ckpt\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note how we didn't have to specify any the hyperparameters again.\n",
    "\n",
    "However, we can easily change some of them by adding them as additional keyword arguments."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[19:40:11] DEBUG: Got {'class_path': 'gnn_tracking.models.graph_construction.GraphConstructionFCNN', 'init_args': {'in_dim': 14, 'hidden_dim': 64, 'out_dim': 8, 'depth': 5, 'beta': 0.4}}, assuming I have to load\u001b[0m\n",
      "\u001b[36m[19:40:11] DEBUG: Getting class GraphConstructionFCNN from module gnn_tracking.models.graph_construction\u001b[0m\n",
      "\u001b[36m[19:40:11] DEBUG: Got obj of type <class 'gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss'>, assuming I have to save hyperparameters\u001b[0m\n",
      "\u001b[36m[19:40:11] DEBUG: Saving hyperperameters {'class_path': 'gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss', 'init_args': {'r_emb': 0.002, 'max_num_neighbors': 5, 'attr_pt_thld': 0.9, 'p_attr': 1, 'p_rep': 1}}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "restored_model_modified = MLModule.load_from_checkpoint(\n",
    "    \"lightning_logs/version_0/checkpoints/epoch=0-step=5.ckpt\",\n",
    "    lw_repulsive=0.1,\n",
    "    loss_fct=GraphConstructionHingeEmbeddingLoss(max_num_neighbors=5),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that you cannot modify the model architecture however (but you could in principle change the `beta` parameter of the residual connections)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running all of this from the command line"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All of the following can be achieved by running the following command:\n",
    "\n",
    "```bash\n",
    "python3 gnn_tracking/trainers/run.py fit --model configs/model.yml --data configs/data.yml  --trainer.accelerator cpu --trainer.accelerator cpu\n",
    "```\n",
    "\n",
    "with the data config file\n",
    "\n",
    "```yaml\n",
    "train:\n",
    "  dirs:\n",
    "    - /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/\n",
    "  stop: 5\n",
    "test:\n",
    "  dirs:\n",
    "    - /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/\n",
    "  star: 10\n",
    "  stop: 15\n",
    "val:\n",
    "  dirs:\n",
    "    - /Users/fuchur/tmp/truth_cut_graphs_for_gsoc/part_1_0/\n",
    "  start: 5\n",
    "  stop: 10\n",
    "```\n",
    "\n",
    "and model config file:\n",
    "\n",
    "```\n",
    "class_path: gnn_tracking.training.ml.MLModule\n",
    "init_args:\n",
    "  model:\n",
    "    class_path: gnn_tracking.models.graph_construction.GraphConstructionFCNN\n",
    "    init_args:\n",
    "      in_dim: 14\n",
    "      out_dim: 8\n",
    "      hidden_dim: 512\n",
    "      depth: 5\n",
    "  lw_repulsive: 0.5\n",
    "  loss_fct:\n",
    "    class_path: gnn_tracking.metrics.losses.GraphConstructionHingeEmbeddingLoss\n",
    "    init_args: {}\n",
    "  optimizer:\n",
    "    class_path: torch.optim.Adam\n",
    "    init_args:\n",
    "      lr: 0.0001\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To quickly override one of the options, you can simply add them to the command line, e.g., `--model.init_args.lw_repulsive=0.1` or `--model.model.init_args.depth=6`."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
